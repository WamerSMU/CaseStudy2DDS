?austourists
# Always plot the data first!
plot(austourists)
# Show the forecast summary
print(forecast_arima)
# Load necessary libraries
library(forecast)
library(readr)
# Assuming 'ausair.csv' contains 'Year' and 'Passengers'
# Replace the path with the correct path to your CSV file
file_path <- "C:/Users/wamer/Downloads/ausair.csv"
# Load the data
ausair_data <- read_csv(file_path)
# Convert the Year column to a numeric type if it's not already
# This assumes 'Year' is in a format that can be directly converted to numeric
ausair_data$Year <- as.numeric(format(as.Date(ausair_data$Year, format="%Y"), "%Y"))
# Creating a time series object from the 'Passengers' column
ausair_ts <- ts(ausair_data$Passengers, start = min(ausair_data$Year), frequency = 1)
# Plot the time series data
plot(ausair_ts, xlab = "Year", ylab = "Passengers", type = "o", main = "Australian Air Passengers")
# Fit an ARIMA model
fit_arima <- auto.arima(ausair_ts)
# Forecasting next 5 years (adjust 'h' for different periods)
forecast_arima <- forecast(fit_arima, h = 5)
# Plot the forecast
autoplot(forecast_arima) + ylab("Passengers") + xlab("Year") + ggtitle("ARIMA Forecast of Australian Air Passengers")
if (!requireNamespace("fpp2", quietly = TRUE)) {
install.packages("fpp2")
}
# Load the fpp2 package
library(fpp2)
# Load the dataset
data(maxtemp)
# Subset maxtemp for years after 1990
maxtemp_post1990 <- window(maxtemp, start=1991)
# Fit a SES model to the subsetted data
ses_model <- ses(maxtemp_post1990)
# Forecast the next five years
ses_forecast <- forecast(ses_model, h=5)
# Plot the original data, SES predictions, and forecast
autoplot(ses_forecast) +
autolayer(fitted(ses_model), series="SES Prediction", col="blue") +
ggtitle("SES Model Forecast") +
xlab("Year") + ylab("Max Temperature")
# Corrected Plotting Code
autoplot(ses_forecast) +
autolayer(fitted(ses_model), series="SES Prediction", colour="blue") +
ggtitle("SES Model Forecast") +
xlab("Year") + ylab("Max Temperature")
# Use the plot function directly from the forecast package
plot(ses_forecast, main="SES Model Forecast", xlab="Year", ylab="Max Temperature")
lines(fitted(ses_model), col="blue")
# Calculate AICc and BIC
AICc_ses <- AIC(ses_model, k = log(length(maxtemp_post1990)))
summary(ses_model)
# Fit a damped Holt's linear trend model to the subsetted data
holt_model <- holt(maxtemp_post1990, damped = TRUE, initial = "optimal")
# Forecast the next five years
holt_forecast <- forecast(holt_model, h=5)
# Plot the original data, Holt predictions, and forecast
plot(holt_forecast, main="Holt's Damped Model Forecast", xlab="Year", ylab="Max Temperature")
lines(fitted(holt_model), col="blue")
summary(holt_model)
# Load necessary libraries
library(tidyverse)
library(DataExplorer)
library(janitor)
library(skimr)
library(caret)
library(GGally)
# Load the dataset
train_data <- read.csv("C:/Users/wamer/OneDrive/Desktop/SMU/SF Final project/house-prices-advanced-regression-techniques/train.csv", stringsAsFactors = FALSE)
# Data Cleaning and Preprocessing -------------------------------------------
# Visualize missing data to decide on the columns to remove or impute
plot_missing(train_data)
# Remove columns with high percentage of missing values
high_na_columns <- c("Alley", "PoolQC", "Fence", "MiscFeature")
train_data <- select(train_data, -all_of(high_na_columns))
# Impute missing values
train_data$FireplaceQu[is.na(train_data$FireplaceQu)] <- 'None'
train_data$LotFrontage[is.na(train_data$LotFrontage)] <- median(train_data$LotFrontage, na.rm = TRUE)
train_data$MasVnrArea[is.na(train_data$MasVnrArea)] <- median(train_data$MasVnrArea, na.rm = TRUE)
train_data$GarageYrBlt[is.na(train_data$GarageYrBlt)] <- 0
# Remove rows where 'Electrical' is missing
train_data <- drop_na(train_data, Electrical)
# Convert categorical columns to factor type
categorical_cols <- c("MSZoning", "Street", "LotShape", "LandContour", "Utilities",
"LotConfig", "LandSlope", "Neighborhood", "Condition1", "Condition2",
"BldgType", "HouseStyle", "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd",
"MasVnrType", "ExterQual", "ExterCond", "Foundation", "BsmtQual",
"BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Heating",
"HeatingQC", "CentralAir", "Electrical", "KitchenQual", "Functional",
"FireplaceQu", "GarageType", "GarageFinish", "GarageQual", "GarageCond",
"PavedDrive", "SaleType", "SaleCondition")
train_data[categorical_cols] <- lapply(train_data[categorical_cols], factor)
# Exploratory Data Analysis (EDA) -------------------------------------------
# Distribution of Sale Prices
ggplot(train_data, aes(x = SalePrice)) +
geom_histogram(binwidth = 20000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribution of Sale Prices", x = "Sale Price ($)", y = "Count")
# Distribution of Ground Living Area
ggplot(train_data, aes(x = GrLivArea)) +
geom_histogram(binwidth = 50, fill = "green", color = "black") +
theme_minimal() +
labs(title = "Distribution of Ground Living Area", x = "Ground Living Area (sq ft)", y = "Count")
# Distribution of Year Built
ggplot(train_data, aes(x = YearBuilt)) +
geom_histogram(binwidth = 5, fill = "orange", color = "black") +
theme_minimal() +
labs(title = "Distribution of Year Built", x = "Year Built", y = "Count")
# Modeling ------------------------------------------------------------------
# Building the linear regression model
analysis1_model <- lm(SalePrice ~ GrLivArea + Neighborhood, data = train_data)
# Summarizing the model
summary(analysis1_model)
# Checking model diagnostics
par(mfrow = c(2, 2))
plot(analysis1_model)
# Log transformation of SalePrice to improve model
train_data$LogSalePrice <- log(train_data$SalePrice)
analysis1_model_log <- lm(LogSalePrice ~ GrLivArea + Neighborhood, data = train_data)
summary(analysis1_model_log)
# Splitting the data into training and testing sets for evaluation
set.seed(123)
index <- sample(1:nrow(train_data), round(0.8 * nrow(train_data)))
train_set <- train_data[index, ]
test_set <- train_data[-index, ]
# Fit the model on the training set
model_train <- lm(LogSalePrice ~ GrLivArea + Neighborhood, data = train_set)
# Predictions and evaluation
predictions <- predict(model_train, test_set)
actual <- test_set$LogSalePrice
predicted <- predictions
RMSE <- sqrt(mean((predicted - actual)^2))
cat("RMSE:", RMSE, "\n")
# Back-transform predictions and actual values
back_transformed_actual <- exp(actual)
back_transformed_predicted <- exp(predicted)
RMSE_original_scale <- sqrt(mean((back_transformed_predicted - back_transformed_actual)^2))
cat("RMSE on original SalePrice scale:", RMSE_original_scale, "\n")
# Load necessary libraries
library(tidyverse)
library(DataExplorer)
library(janitor)
library(skimr)
library(caret)
library(GGally)
# Load the dataset
train_data <- read.csv("C:/Users/wamer/OneDrive/Desktop/SMU/SF Final project/house-prices-advanced-regression-techniques/train.csv", stringsAsFactors = FALSE)
# Data Cleaning and Preprocessing
# Visualize missing data to decide on the columns to remove or impute
plot_missing(train_data)
# Remove columns with high percentage of missing values
high_na_columns <- c("Alley", "PoolQC", "Fence", "MiscFeature")
train_data <- select(train_data, -all_of(high_na_columns))
# Impute missing values
train_data$FireplaceQu[is.na(train_data$FireplaceQu)] <- 'None'
train_data$LotFrontage[is.na(train_data$LotFrontage)] <- median(train_data$LotFrontage, na.rm = TRUE)
train_data$MasVnrArea[is.na(train_data$MasVnrArea)] <- median(train_data$MasVnrArea, na.rm = TRUE)
train_data$GarageYrBlt[is.na(train_data$GarageYrBlt)] <- 0
# Remove rows where 'Electrical' is missing
train_data <- drop_na(train_data, Electrical)
# Convert categorical columns to factor type
categorical_cols <- c("MSZoning", "Street", "LotShape", "LandContour", "Utilities",
"LotConfig", "LandSlope", "Neighborhood", "Condition1", "Condition2",
"BldgType", "HouseStyle", "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd",
"MasVnrType", "ExterQual", "ExterCond", "Foundation", "BsmtQual",
"BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Heating",
"HeatingQC", "CentralAir", "Electrical", "KitchenQual", "Functional",
"FireplaceQu", "GarageType", "GarageFinish", "GarageQual", "GarageCond",
"PavedDrive", "SaleType", "SaleCondition")
train_data[categorical_cols] <- lapply(train_data[categorical_cols], factor)
# Exploratory Data Analysis (EDA)
# Distribution of Sale Prices
ggplot(train_data, aes(x = SalePrice)) +
geom_histogram(binwidth = 20000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribution of Sale Prices", x = "Sale Price ($)", y = "Count")
# Distribution of Ground Living Area
ggplot(train_data, aes(x = GrLivArea)) +
geom_histogram(binwidth = 50, fill = "green", color = "black") +
theme_minimal() +
labs(title = "Distribution of Ground Living Area", x = "Ground Living Area (sq ft)", y = "Count")
# Distribution of Year Built
ggplot(train_data, aes(x = YearBuilt)) +
geom_histogram(binwidth = 5, fill = "orange", color = "black") +
theme_minimal() +
labs(title = "Distribution of Year Built", x = "Year Built", y = "Count")
# Modeling
# Focusing on neighborhoods of interest
focused_data <- filter(train_data, Neighborhood %in% c("NAmes", "Edwards", "BrkSide"))
# Building the linear regression model for original SalePrice
analysis1_model <- lm(SalePrice ~ GrLivArea + Neighborhood, data = focused_data)
# Checking model diagnostics for original SalePrice
par(mfrow = c(2, 2))
plot(analysis1_model)
# Log transformation of SalePrice to improve model fitting
focused_data$LogSalePrice <- log(focused_data$SalePrice)
# Building the linear regression model for log-transformed SalePrice
analysis1_model_log <- lm(LogSalePrice ~ GrLivArea + Neighborhood, data = focused_data)
summary(analysis1_model_log)
# Checking model diagnostics for log-transformed SalePrice
par(mfrow = c(2, 2))
plot(analysis1_model_log)
# Confidence Intervals for both models
confint(analysis1_model, level = 0.95)
confint(analysis1_model_log, level = 0.95)
# To ensure a comprehensive answer for your specific scenario, let's calculate the increase in sale price per 100 sq. ft increment.
focused_data$GrLivArea100 <- focused_data$GrLivArea / 100
analysis1_model_adj <- lm(LogSalePrice ~ GrLivArea100 + Neighborhood, data = focused_data)
summary(analysis1_model_adj)
confint(analysis1_model_adj, level = 0.95)
library(MASS) # Load MASS for stepAIC
# Define the full model and null model
full_model <- lm(SalePrice ~ ., data = train_data) # All predictors
null_model <- lm(SalePrice ~ 1, data = train_data) # No predictors
# Forward Selection
forward_model <- stepAIC(null_model, scope = list(lower = null_model, upper = full_model),
direction = "forward", trace = FALSE)
# Recheck for any missing values
sum(is.na(train_data))
# If there are any new missing values, you'll need to handle them again
train_data <- na.omit(train_data) # or another appropriate method
# Redefine your full and null models with the cleaned dataset
full_model <- lm(SalePrice ~ ., data = train_data)
null_model <- lm(SalePrice ~ 1, data = train_data)
# Try forward selection again
forward_model <- stepAIC(null_model, scope = list(lower = null_model, upper = full_model),
direction = "forward", trace = FALSE)
# Check the summary of the new model
summary(forward_model)
# Backward Elimination
backward_model <- stepAIC(full_model, direction = "backward", trace = FALSE)
summary(backward_model)
# Stepwise Selection (both directions)
stepwise_model <- stepAIC(null_model, scope = list(lower = null_model, upper = full_model),
direction = "both", trace = FALSE)
summary(stepwise_model)
# Calculate VIF for the model to check for multicollinearity
library(car)
# Assuming 'model' is your current regression model
vif_model <- vif(model)
# Calculate VIF for the forward selection model
vif_forward_model <- vif(forward_model)
print(vif_forward_model)
# Load necessary libraries
library(tidyr)
library(dplyr)
library(caret)
# Load the dataset
data <- read.csv("C:/Users/wamer/OneDrive/Desktop/SMU/SF Final project/house-prices-advanced-regression-techniques/train.csv")
# Check for missing values and remove columns with more than 80% missing
threshold <- 0.8
data <- data[, -which(sapply(data, function(x) sum(is.na(x)) / length(x)) > threshold)]
# For numerical columns with missing values, impute with median
numerical_features <- sapply(data, is.numeric)
data[, numerical_features] <- lapply(data[, numerical_features], function(x) {
ifelse(is.na(x), median(x, na.rm = TRUE), x)
})
# For categorical columns with missing values, impute with the most frequent level
categorical_features <- sapply(data, is.factor)
data[, categorical_features] <- lapply(data[, categorical_features], function(x) {
levels <- levels(x)
x[is.na(x)] <- levels[which.max(tabulate(match(x, levels)))]
return(x)
})
# Convert categorical variables to factors
data[, sapply(data, is.character)] <- lapply(data[, sapply(data, is.character)], as.factor)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
index <- createDataPartition(data$SalePrice, p=0.8, list=FALSE)
trainData <- data[index, ]
testData <- data[-index, ]
# Summary to check if all is good before modeling
summary(trainData)
# Load the MASS library for stepAIC
library(MASS)
# Define the full model with all predictors
fullModel <- lm(SalePrice ~ ., data = trainData)
# Define the null model with no predictors
nullModel <- lm(SalePrice ~ 1, data = trainData)
# Use stepAIC to perform forward selection
forwardModel <- stepAIC(nullModel, scope = list(lower = nullModel, upper = fullModel),
direction = "forward", trace = FALSE)
library(shiny)
runGitHub(repo="App.R", username="WamerSMU", ref="main")
shiny::runGitHub(repo = "CaseStudy2DDS", username = "WamerSMU", ref = "main")
library(tidyverse)
library(dplyr)
library(GGally)
library(ggplot2)
library(ggthemes)
library(plotly)
library(class)
library(caret)
library(e1071)
library(readr)
library(reshape2)
library(rmarkdown)
CaseStudy2 <- read_csv("C:/Users/wamer/OneDrive/Desktop/SMU/DDS Final/CaseStudy2-data.csv")
###EDA
#Exploring the factor of Job satisfaction, Age, and monthly income on Attrition
#Monthly Income seems to be a big indicator of attrition
#Age might play a role as well
ggplot(data = CaseStudy2) +
geom_point(mapping = aes(x = Age, y = MonthlyIncome, color = Attrition)) +
geom_smooth(mapping = aes(x = Age, y = MonthlyIncome, linetype = Attrition, color = Attrition)) + facet_grid(~JobSatisfaction)
CaseStudy2 %>% ggplot(mapping = aes(x = Department, y = DistanceFromHome, color= Attrition)) + geom_point(position = "jitter")
p = CaseStudy2 %>%
ggplot(mapping = aes(x = Department, y = MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") + ggtitle("Department vs MI") + theme_economist()
p1 = CaseStudy2 %>%
ggplot(mapping = aes(x = Age, y = MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") + ggtitle("Department vs MI") + theme_economist()
p2 = CaseStudy2 %>%
ggplot(mapping = aes(x = MaritalStatus, y = MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") + ggtitle("Department vs MI") + theme_economist()
p3 = CaseStudy2 %>%
ggplot(mapping = aes(x = JobLevel, y = MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") + ggtitle("Department vs MI") + theme_economist()
p4 = CaseStudy2 %>%
ggplot(mapping = aes(x = JobSatisfaction, y = MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") + ggtitle("Department vs MI") + theme_economist()
p5 = CaseStudy2 %>%
ggplot(mapping = aes(x = Gender, y = MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") + ggtitle("Department vs MI") + theme_economist()
### Naive Bayes Age and Monthly Income
CaseStudy2$Age <- as.factor(CaseStudy2$Age)  # Ensuring Age is treated as a factor
model_Age_MonthlyIncome <- naiveBayes(as.factor(CaseStudy2$Attrition) ~ Age + MonthlyIncome, data = CaseStudy2, laplace = 1)
predictions_Age_MonthlyIncome <- predict(model_Age_MonthlyIncome, CaseStudy2[, c("Age", "MonthlyIncome")])
CM1_Age_MonthlyIncome <- confusionMatrix(as.factor(predictions_Age_MonthlyIncome), as.factor(CaseStudy2$Attrition))
print(CM1_Age_MonthlyIncome)
### Naive Bayes Job Satisfaction, Monthly Income, and Age
CaseStudy2$JobSatisfaction <- as.factor(CaseStudy2$JobSatisfaction)  # Ensure JobSatisfaction is treated as a factor
model_JobSatisfaction_Age_MonthlyIncome <- naiveBayes(as.factor(CaseStudy2$Attrition) ~ JobSatisfaction + Age + MonthlyIncome, data = CaseStudy2, laplace = 1)
predictions_JobSatisfaction_Age_MonthlyIncome <- predict(model_JobSatisfaction_Age_MonthlyIncome, CaseStudy2[, c("JobSatisfaction", "Age", "MonthlyIncome")])
CM1_JobSatisfaction_Age_MonthlyIncome <- confusionMatrix(as.factor(predictions_JobSatisfaction_Age_MonthlyIncome), as.factor(CaseStudy2$Attrition))
print(CM1_JobSatisfaction_Age_MonthlyIncome)
### Naive Bayes Job Level, Monthly Income, and Age
CaseStudy2$JobLevel <- as.factor(CaseStudy2$JobLevel)  # Ensure JobLevel is treated as a factor
model_JobLevel_Age_MonthlyIncome <- naiveBayes(as.factor(CaseStudy2$Attrition) ~ JobLevel + Age + MonthlyIncome, data = CaseStudy2, laplace = 1)
predictions_JobLevel_Age_MonthlyIncome <- predict(model_JobLevel_Age_MonthlyIncome, CaseStudy2[, c("JobLevel", "Age", "MonthlyIncome")])
CM1_JobLevel_Age_MonthlyIncome <- confusionMatrix(as.factor(predictions_JobLevel_Age_MonthlyIncome), as.factor(CaseStudy2$Attrition))
print(CM1_JobLevel_Age_MonthlyIncome)
###KNN Doesn't work
data <- read_csv("C:/Users/wamer/OneDrive/Desktop/SMU/DDS Final/CaseStudy2-data.csv")
data$Attrition <- factor(ifelse(data$Attrition == "Yes", 1, 0), levels = c(0, 1))
table(data$Attrition)
normalized_features <- scale(data[, c("Age", "MonthlyIncome")])
norm_data <- data
norm_data[, c("Age", "MonthlyIncome")] <- normalized_features
set.seed(123)
index <- createDataPartition(y = data$Attrition, p = 0.80, list = TRUE, times = 1)
train <- norm_data[index[[1]], ]
test <- norm_data[-index[[1]], ]
train_labels <- data$Attrition[index[[1]]]
test_labels <- data$Attrition[-index[[1]]]
if (any(table(train_labels) == 0) | any(table(test_labels) == 0)) {
cat("Repartitioning due to unrepresented class\n")
# Further steps to manually adjust the partition to ensure both classes are represented
}
ctrl <- trainControl(method = "cv", number = 10)
# Train KNN
knn_fit <- train(x = train[, c("Age", "MonthlyIncome")], y = train_labels, method = "knn", tuneLength = 10, trControl = ctrl)
# Predict using the best model
predictions <- predict(knn_fit, newdata = test)
# Evaluate the model
conf_mat <- confusionMatrix(predictions, test_labels)
print(conf_mat)
###Linear Regression
# Define the model fitting and diagnosing function
fit_and_diagnose <- function(formula, data) {
fit <- lm(formula, data = data)
summary(fit)
par(mfrow = c(2, 2))
plot(fit)
return(fit)
}
# Variables 'MonthlyIncome' and 'JobSatisfaction'
simple_model <- lm(MonthlyIncome ~ JobSatisfaction, data = CaseStudy2)
summary(simple_model)
# 'Attrition' as a predictor along with 'JobSatisfaction'
multiple_model <- lm(MonthlyIncome ~ JobSatisfaction + Attrition, data = CaseStudy2)
summary(multiple_model)
# To test if the impact of 'JobSatisfaction' on 'MonthlyIncome' is different for those with 'Attrition' being 'Yes' or 'No'
interaction_model <- lm(MonthlyIncome ~ JobSatisfaction * Attrition, data = CaseStudy2)
summary(interaction_model)
### Monthly Income Attrition + Job Satisfaction Grid
groupedMI = CaseStudy2 %>% group_by(MonthlyIncome, Attrition)
groupedMImean = groupedMI %>% summarise(mean(JobSatisfaction))
CaseStudy2 %>% ggplot(aes(JobSatisfaction, MonthlyIncome, color = Attrition)) +
geom_point(position = "jitter") +
facet_grid(Attrition~JobSatisfaction) +
ggtitle("Job Satisfaction vs MonthlyIncome by Attrition")
### Naive Bayes Threshold Adjustment
# Predicting class probabilities
predicted_probs <- predict(model_Age_MonthlyIncome, CaseStudy2, type = "raw")
# Defining a sequence of thresholds to test
thresholds <- seq(0.1, 0.9, by = 0.1)
# Empty vector to store balanced accuracy
balanced_accuracy <- numeric(length(thresholds))
# Ensure Attrition is a factor with the correct levels
CaseStudy2$Attrition <- factor(CaseStudy2$Attrition, levels = c("No", "Yes"))
# Looping through thresholds to calculate balanced accuracy
for(i in seq_along(thresholds)){
threshold <- thresholds[i]
predictions <- ifelse(predicted_probs[,2] > threshold, "Yes", "No")
# Ensure that the predictions are factors with the correct levels
predictions <- factor(predictions, levels = c("No", "Yes"))
cm <- confusionMatrix(predictions, CaseStudy2$Attrition)
sensitivity <- cm$byClass['Sensitivity']
specificity <- cm$byClass['Specificity']
balanced_accuracy[i] <- (sensitivity + specificity) / 2
}
# Find the threshold with the highest balanced accuracy
best_threshold <- thresholds[which.max(balanced_accuracy)]
best_threshold
# Applying the chosen threshold to generate new predictions
CaseStudy2$Predicted_Attrition <- ifelse(predicted_probs[, "Yes"] > 0.2, "Yes", "No")
CaseStudy2$Predicted_Attrition <- factor(CaseStudy2$Predicted_Attrition, levels = c("No", "Yes"))
# Calculating the confusion matrix with the new predictions
new_cm <- confusionMatrix(CaseStudy2$Predicted_Attrition, CaseStudy2$Attrition)
# Printing the confusion matrix and related metrics
print(new_cm)
# Apply the chosen threshold to the probabilities to make final predictions
CaseStudy2$Final_Predicted_Attrition <- ifelse(predicted_probs[, "Yes"] > best_threshold, "Yes", "No")
#####
# Load necessary libraries
library(corrplot)
library(MASS)
library(caret)
# Assuming 'data' is your dataset
# First, refine the dataset to exclude non-numeric columns as well as columns with zero variance
numeric_data <- data[, sapply(data, is.numeric)]
numeric_data_refined <- numeric_data[, apply(numeric_data, 2, var) != 0]
# Exclude specific columns known to be irrelevant or constant
excluded_columns <- c("StandardHours", "EmployeeCount", "EmployeeNumber")
numeric_data_refined <- numeric_data_refined[, !(names(numeric_data_refined) %in% excluded_columns)]
# Calculate the correlation matrix, handling NA values by specifying use = "complete.obs"
cor_matrix <- cor(numeric_data_refined, use = "complete.obs")
# Visualize the correlation matrix
corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
data$Department <- as.factor(data$Department)
# Load the dataset
data_path <- "C:/Users/wamer/OneDrive/Desktop/SMU/DDS Final/CaseStudy2-data.csv"
data <- read.csv(data_path)
# Starting model: Intercept only
start.model <- lm(MonthlyIncome ~ 1, data = data)
# Maximum model: Includes all potential predictors, now adding JobLevel and TotalWorkingYears
max.model <- lm(MonthlyIncome ~ Age + Department + YearsAtCompany + JobLevel + TotalWorkingYears, data = data)
# Perform forward selection
forward.model <- stepAIC(start.model, direction = "forward",
scope = list(lower = start.model, upper = max.model),
trace = FALSE)
# Display the summary of the selected model
summary(forward.model)
# Define control method: 10-fold CV with RMSE
trainControl <- trainControl(method = "cv", number = 10, summaryFunction = defaultSummary)
# Model training with cross-validation focusing on RMSE, now including JobLevel and TotalWorkingYears
model_cv <- train(MonthlyIncome ~ Age + Department + YearsAtCompany + JobLevel + TotalWorkingYears, data = data,
method = "lm",
trControl = trainControl,
metric = "RMSE")
# Print the results, including RMSE
print(model_cv)
# Evaluate if the RMSE goal of < $3000 is met
if(model_cv$results$RMSE < 3000) {
cat("Success: The model's RMSE of", model_cv$results$RMSE, "meets the goal of being less than $3000.\n")
} else {
cat("Attention: The model's RMSE of", model_cv$results$RMSE, "does not meet the goal of being less than $3000.\n")
}
library(shiny)
library(ggplot2)
library(dplyr)
# Define UI
ui <- fluidPage(
titlePanel("Employee Data Exploration"),
sidebarLayout(
sidebarPanel(
sliderInput("incomeRange", "Monthly Income Range:",
min = min(CaseStudy2$MonthlyIncome, na.rm = TRUE),
max = max(CaseStudy2$MonthlyIncome, na.rm = TRUE),
value = range(CaseStudy2$MonthlyIncome, na.rm = TRUE)),
selectInput("jobSatisfaction", "Job Satisfaction Level:",
choices = unique(CaseStudy2$JobSatisfaction),
selected = unique(CaseStudy2$JobSatisfaction)[1])
),
mainPanel(
plotOutput("incomePlot")
)
)
)
# Define server logic
server <- function(input, output) {
output$incomePlot <- renderPlot({
filtered <- CaseStudy2 %>%
filter(MonthlyIncome >= input$incomeRange[1], MonthlyIncome <= input$incomeRange[2],
JobSatisfaction == input$jobSatisfaction)
ggplot(filtered, aes(x = Age, y = MonthlyIncome, color = Attrition)) +
geom_point() +
labs(title = "Monthly Income vs. Age", x = "Age", y = "Monthly Income")
})
}
# Run the application
shinyApp(ui = ui, server = server)
install.packages("rsconnect")
rsconnect::setAccountInfo(name='waleedamer', token='F252C8E94C3DF8BC5A30EA8232E71E39', secret='unrK839+/DPmVUuZKq7Z96P+uTmeqBqcTp64mX9X')
setwd("C:/Users/wamer/Desktop/GitHub Repository/CaseStudy2DDS")
setwd("C:/Users/wamer/Desktop/GitHub Repository/CaseStudy2DDS")
rsconnect::deployApp()
